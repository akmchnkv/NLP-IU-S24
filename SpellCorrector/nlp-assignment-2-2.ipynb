{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3167333,"sourceType":"datasetVersion","datasetId":1887500},{"sourceId":7799698,"sourceType":"datasetVersion","datasetId":4566821},{"sourceId":7879194,"sourceType":"datasetVersion","datasetId":4624329},{"sourceId":6368605,"sourceType":"datasetVersion","datasetId":3669187}],"dockerImageVersionId":30664,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Context-sensitive Spelling Correction\n\nThe goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n\nSubmit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n\nUseful links:\n- [Norvig's solution](https://norvig.com/spell-correct.html)\n- [Norvig's dataset](https://norvig.com/big.txt)\n- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n\nGrading:\n- 60 points - Implement spelling correction\n- 20 points - Justify your decisions\n- 20 points - Evaluate on a test set\n","metadata":{"id":"DIgM6C9HYUhm"}},{"cell_type":"markdown","source":"## Implement context-sensitive spelling correction\n\nYour task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n\nThe best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n\nYou may also want to implement:\n- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n- some recent (or not very recent) paper on this topic,\n- solution which takes into account keyboard layout and associated misspellings,\n- efficiency improvement to make the solution faster,\n- any other idea of yours to improve the Norvigâ€™s solution.\n\nIMPORTANT:  \nYour project should not be a mere code copy-paste from somewhere. You must provide:\n- Your implementation\n- Analysis of why the implemented approach is suggested\n- Improvements of the original approach that you have chosen to implement","metadata":{"id":"x-vb8yFOGRDF"}},{"cell_type":"markdown","source":"## Norvig's solution ","metadata":{}},{"cell_type":"code","source":"import re\nfrom collections import Counter\n\ndef words(text): return re.findall(r'\\w+', text.lower())\n\nfile = open('fivegrams.txt', encoding='latin-1').read()\nWORDS = Counter(words(file))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T08:49:39.860447Z","iopub.execute_input":"2024-03-22T08:49:39.861282Z","iopub.status.idle":"2024-03-22T08:49:43.100416Z","shell.execute_reply.started":"2024-03-22T08:49:39.861245Z","shell.execute_reply":"2024-03-22T08:49:43.099593Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class NorvigSpellCorrector:\n    def __init__(self):\n        self.letters = 'abcdefghijklmnopqrstuvwxyz'\n        self.WORDS = WORDS\n    \n    def probability(self, word, N=None):\n        \"\"\"Calculate the probability of `word`.\"\"\"\n        if N is None:\n            N = sum(self.WORDS.values())\n        return self.WORDS[word] / N\n    \n    def known(self, words):\n        \"\"\"The subset of `words` that appear in the dictionary of WORDS.\"\"\"\n        return set(w for w in words if w in self.WORDS)\n    \n    def edits1(self, word):\n        \"All edits that are one edit away from `word`.\"\n        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n        deletes    = [L + R[1:]               for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n        replaces   = [L + c + R[1:]           for L, R in splits if R for c in self.letters]\n        inserts    = [L + c + R               for L, R in splits for c in self.letters]\n        return set(deletes + transposes + replaces + inserts)\n    \n    def edits2(self, word):\n        \"\"\"All edits that are two edits away from `word`.\"\"\"\n        return (e2 for e1 in self.edits1(word) for e2 in self.edits1(e1))\n\n    def candidates(self, word):\n        \"\"\"Generate possible spelling corrections for word.\"\"\"\n        return (self.known([word]) | self.known(self.edits1(word)) | self.known(self.edits2(word)) | {word})\n\n    def correction(self, word):\n        \"\"\"Most probable spelling correction for word.\"\"\"\n        return max(self.candidates(word), key=self.probability)\n\n    def correct_sentence(self, sentence):\n        \"\"\"Corrects all words within a sentence.\"\"\"\n        return ' '.join(self.correction(word) for word in re.findall(r'\\w+', sentence.lower()))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T08:49:43.101958Z","iopub.execute_input":"2024-03-22T08:49:43.102257Z","iopub.status.idle":"2024-03-22T08:49:43.115231Z","shell.execute_reply.started":"2024-03-22T08:49:43.102221Z","shell.execute_reply":"2024-03-22T08:49:43.114301Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# My solution","metadata":{}},{"cell_type":"code","source":"from collections import defaultdict\n\ndef update_ngram_counts(ngram_dict, ngram_tuple, increment):\n    \"\"\"\n    Updates the count of an n-gram in a dictionary.\n\n    Parameters:\n    - ngram_dict (dict): Dictionary of n-gram counts.\n    - ngram_tuple (tuple): The n-gram tuple to update.\n    - increment (int): The value to add to the n-gram's count.\n\n    Raises:\n    - ValueError: If attempting to add a duplicate n-gram with no increment.\n    \"\"\"\n    if ngram_dict[ngram_tuple] and increment == 0:\n        raise ValueError(f\"Duplicate detected for n-gram: {ngram_tuple}\")\n    ngram_dict[ngram_tuple] += increment\n\ndef extract_ngrams_and_update(line, gram5_dict, gram4_dict, gram3_dict, is_five_gram=True):\n    \"\"\"\n    Extracts n-grams from a line of text and updates corresponding n-gram dictionaries.\n\n    Parameters:\n    - line (str): A line of text containing an n-gram and its count.\n    - gram5_dict, gram4_dict, gram3_dict (dict): Dictionaries for 5-gram, 4-gram, and 3-gram counts.\n    - is_five_gram (bool): True if processing a 5-gram, False if processing a 2-gram.\n\n    Processes the line to extract the n-gram and its count, updates the 5-gram dictionary if applicable,\n    and also updates 4-gram and 3-gram dictionaries based on the extracted n-gram.\n    \"\"\"\n    parts = line.strip().split(\"\\t\")\n    base_ngram = tuple(parts[1:])\n    count = int(parts[0])\n\n    if is_five_gram:\n        update_ngram_counts(gram5_dict, base_ngram, count if gram5_dict[base_ngram] == 0 else 0)\n        sub_ngrams = [base_ngram[1:], base_ngram[:-1]]  # 4-grams from 5-gram\n    else:\n        sub_ngrams = [base_ngram]\n\n    # Update 4-gram and 3-gram dictionaries based on extracted n-grams\n    for ngram in sub_ngrams:\n        if len(ngram) == 4:\n            update_ngram_counts(gram4_dict, ngram, count)\n            # Generate and update 3-grams from 4-grams\n            for i in range(1, 4):\n                update_ngram_counts(gram3_dict, ngram[i-1:i+2], count)\n        elif len(ngram) == 2 and not is_five_gram:\n            update_ngram_counts(gram2_dict, ngram, count)\n\ngram5_dict, gram4_dict, gram3_dict, gram2_dict = [defaultdict(int) for _ in range(4)]\n\n# Process 5-gram file\nwith open('fivegrams.txt', 'r', encoding='latin-1') as file:\n    [extract_ngrams_and_update(line, gram5_dict, gram4_dict, gram3_dict) for line in file]\n\n# Process 2-gram file\nwith open('bigrams.txt', 'r', encoding='latin-1') as file:\n    [extract_ngrams_and_update(line, gram2_dict, gram4_dict, gram3_dict, is_five_gram=False) for line in file]\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T08:49:43.116609Z","iopub.execute_input":"2024-03-22T08:49:43.116888Z","iopub.status.idle":"2024-03-22T08:50:00.224600Z","shell.execute_reply.started":"2024-03-22T08:49:43.116856Z","shell.execute_reply":"2024-03-22T08:50:00.223754Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def print_top_ngrams(ngram_dict, ngram_name, top_n=5):\n    \"\"\"Print the top N most common n-grams and their counts.\"\"\"\n    top_ngrams = sorted(ngram_dict.items(), key=lambda x: x[1], reverse=True)[:top_n]\n    print(f\"------- {ngram_name}: -------\")\n    print(f\"Top {top_n} most common {ngram_name.lower()}:\")\n    for ngram, count in top_ngrams:\n        print(f\"{' '.join(ngram)}: {count}\")\n    print() \n\n\nprint_top_ngrams(gram5_dict, \"Fivegrams\")\nprint_top_ngrams(gram4_dict, \"Fourgrams\")\nprint_top_ngrams(gram3_dict, \"Threegrams\")\nprint_top_ngrams(gram2_dict, \"Twograms\")","metadata":{"execution":{"iopub.status.busy":"2024-03-22T08:50:00.226613Z","iopub.execute_input":"2024-03-22T08:50:00.226917Z","iopub.status.idle":"2024-03-22T08:50:02.098303Z","shell.execute_reply.started":"2024-03-22T08:50:00.226891Z","shell.execute_reply":"2024-03-22T08:50:02.097293Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"------- Fivegrams: -------\nTop 5 most common fivegrams:\nat the end of the: 13588\ni do n't want to: 12744\nin the middle of the: 9124\ni do n't know what: 8076\nyou do n't have to: 7186\n\n------- Fourgrams: -------\nTop 5 most common fourgrams:\nthe end of the: 53263\ndo n't want to: 50903\ni do n't think: 50304\ni do n't know: 42473\nin the united states: 39050\n\n------- Threegrams: -------\nTop 5 most common threegrams:\nof the: 1253466\nin the: 372495\nto be: 306868\nto the: 243312\ni do n't: 230390\n\n------- Twograms: -------\nTop 5 most common twograms:\nof the: 2586813\nin the: 2043262\nto the: 1055301\non the: 920079\nand the: 737714\n\n","output_type":"stream"}]},{"cell_type":"code","source":"class ContextSearcher:\n    def __init__(self, ngram_maps):\n        \"\"\"\n        Initializes the ContextSearcher with a dictionary of n-gram maps.\n\n        Parameters:\n        - ngram_maps (dict): A dictionary where keys are integers indicating the n-gram size (e.g., 2 for bigrams, 3 for trigrams, etc.) \n                             and values are dictionaries mapping n-gram tuples to their occurrence counts in a corpus.\n        \"\"\"\n        self.ngram_maps = ngram_maps \n\n    def get_context_score(self, word_sequence):\n        \"\"\"\n        Retrieves the occurrence count (score) of a specific word sequence from the n-gram maps.\n\n        Parameters:\n        - word_sequence (tuple): A tuple of words representing the n-gram whose score is to be retrieved.\n\n        Returns:\n        int: The occurrence count of the given word sequence in the corpus. Returns 0 if the sequence is not found.\n        \"\"\"\n        ngram_level = len(word_sequence)\n        if ngram_level in self.ngram_maps:\n            return self.ngram_maps[ngram_level].get(word_sequence, 0)\n        return 0\n\n    def find_contexts(self, target, precursors):\n        \"\"\"\n        Identifies and scores contexts in which a target word appears, based on the precursor words provided.\n\n        Parameters:\n        - target (str): The target word for which contexts are being searched.\n        - precursors (list): A list of words preceding the target word, used to construct n-grams for context search.\n                             The list may contain more than 4 words, but only the last 4 are considered for up to a 5-gram context.\n\n        Returns:\n        list: A list of tuples, where each tuple contains two elements:\n              1. The length of the context (n-1, where n is the n-gram size),\n              2. The occurrence count (score) of the context in the corpus.\n              The list is sorted by context length, with longer contexts appearing first.\n        \"\"\"\n        precursors = precursors[-4:]  # Use up to 4 preceding words for a 5-gram context\n        context_results = []\n\n        for i in range(1, len(precursors) + 1):\n            # Construct the sequence to check, ensuring it ends with the target word\n            word_seq = tuple(precursors[-i:] + [target])\n            score = self.get_context_score(word_seq)\n            if score > 0:\n                context_results.append((len(word_seq) - 1, score))\n\n        # If no context found, attempt to get score for just the target word\n        if not context_results:\n            score = self.get_context_score((target,))\n            if score > 0:\n                context_results.append((1, score))\n\n        return context_results","metadata":{"execution":{"iopub.status.busy":"2024-03-22T08:50:02.099666Z","iopub.execute_input":"2024-03-22T08:50:02.100079Z","iopub.status.idle":"2024-03-22T08:50:02.111419Z","shell.execute_reply.started":"2024-03-22T08:50:02.100043Z","shell.execute_reply":"2024-03-22T08:50:02.110429Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class WordRefiner:\n    def __init__(self, context_searcher):\n        \"\"\"\n        Initializes the WordRefiner with a context searcher for context-based spelling correction.\n\n        Parameters:\n        - context_searcher (ContextSearcher): An instance of the ContextSearcher class used to find contexts\n          and scores for potential corrections.\n        \"\"\"\n        self.searcher = context_searcher\n        self.WORDS = WORDS  \n    \n    def P(self, word, N=None):\n        \"\"\"Probability of `word`.\"\"\"\n        if N is None:\n            N = sum(self.WORDS.values())\n        return self.WORDS[word] / N\n    \n    def known(self, words):\n        \"\"\"The subset of `words` that appear in the dictionary of WORDS.\"\"\"\n        return set(w for w in words if w in self.WORDS)\n\n    def edits1(self, word):\n        \"\"\"All edits that are one edit away from `word`.\"\"\"\n        letters = 'abcdefghijklmnopqrstuvwxyz'\n        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n        inserts = [L + c + R for L, R in splits for c in letters]\n        return set(deletes + transposes + replaces + inserts)\n    \n    def candidates(self, word):\n        \"\"\"Generate possible spelling corrections for word.\"\"\"\n        return self.known([word]) | self.known(self.edits1(word)) | {word}\n\n    def refine_word(self, incorrect_word, context_words):\n        \"\"\"\n        Refines an incorrect word using the surrounding context for the best fit.\n\n        Parameters:\n        - incorrect_word (str): The word to refine.\n        - context_words (list): A list of words preceding the word to refine, providing context.\n\n        Returns:\n        str: The most likely correction of the incorrect word based on context.\n        \"\"\"\n        candidate_words = list(self.candidates(incorrect_word))\n        context_scores = [self.searcher.find_contexts(cand, context_words) for cand in candidate_words]\n\n        best_scores = [sorted(scores, key=lambda x: (-x[0], -x[1]))[0] if scores else (0, 0) for scores in context_scores]\n        best_candidate_index = best_scores.index(max(best_scores, key=lambda x: (x[0], x[1])))\n\n        return candidate_words[best_candidate_index]\n    \n    def correct_sentence(self, sentence):\n        \"\"\"\n        Corrects all words within a sentence based on their context, refining each word individually.\n\n        Parameters:\n        - sentence (str): The sentence to correct.\n\n        Returns:\n        str: The corrected sentence, with each word refined based on context.\n        \"\"\"\n        words = sentence.split()  # Split sentence into words\n        corrected_words = []\n\n        for i in range(len(words)):\n            # Use preceding words as context for the current word\n            context = corrected_words[-4:] if i >= 4 else corrected_words\n            corrected_word = self.refine_word(words[i], context)\n            corrected_words.append(corrected_word)\n        \n        return ' '.join(corrected_words)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-22T08:50:29.024623Z","iopub.execute_input":"2024-03-22T08:50:29.025099Z","iopub.status.idle":"2024-03-22T08:50:29.041765Z","shell.execute_reply.started":"2024-03-22T08:50:29.025067Z","shell.execute_reply":"2024-03-22T08:50:29.040739Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"ngram_maps = {2: gram2_dict, 3: gram3_dict, 4: gram4_dict, 5: gram5_dict}\n\nspell_corrector = NorvigSpellCorrector()\ncontext_searcher = ContextSearcher(ngram_maps)\nword_refiner = WordRefiner(context_searcher)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T08:50:38.025195Z","iopub.execute_input":"2024-03-22T08:50:38.025910Z","iopub.status.idle":"2024-03-22T08:50:38.030718Z","shell.execute_reply.started":"2024-03-22T08:50:38.025880Z","shell.execute_reply":"2024-03-22T08:50:38.029637Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"sentence = \"spellng errurs sometims vary bas\"\n\nprint(\"Original sentence:\", sentence)\nprint()\nprint(\"Corrected sentence by Norvings corrector:\", spell_corrector.correct_sentence(sentence))\nprint(\"Corrected sentence by Context Sensetive corrector:\", word_refiner.correct_sentence(sentence))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T08:50:40.056276Z","iopub.execute_input":"2024-03-22T08:50:40.056615Z","iopub.status.idle":"2024-03-22T08:50:40.641628Z","shell.execute_reply.started":"2024-03-22T08:50:40.056590Z","shell.execute_reply":"2024-03-22T08:50:40.640687Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Original sentence: spellng errurs sometims vary bas\n\nCorrected sentence by Norvings corrector: selling error sometimes are a\nCorrected sentence by Context Sensetive corrector: spelling errors sometimes very bad\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## One more example...","metadata":{}},{"cell_type":"code","source":"word = 'nic'\ncontext_sent = 'Her smile is very'\n\nprint(\"Context:\", context_sent)\nprint(\"Corrected word by Norvings corrector:\", spell_corrector.correction(word))\nprint(\"Corrected word by Context Sensetive corrector:\", word_refiner.refine_word(word, words(context_sent)))","metadata":{"execution":{"iopub.status.busy":"2024-03-22T08:50:49.410096Z","iopub.execute_input":"2024-03-22T08:50:49.410439Z","iopub.status.idle":"2024-03-22T08:50:49.529902Z","shell.execute_reply.started":"2024-03-22T08:50:49.410413Z","shell.execute_reply":"2024-03-22T08:50:49.529006Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Context: Her smile is very\nCorrected word by Norvings corrector: in\nCorrected word by Context Sensetive corrector: nice\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Justification","metadata":{"id":"oML-5sJwGRLE"}},{"cell_type":"markdown","source":"## Contextual Awareness\n\n#### Key Improvement\nIncorporation of context for choosing corrections.\n\n#### Justification\nWhile Norvig's model efficiently identifies potential corrections based on edit distance, it doesn't account for the contextual appropriateness of a word. By integrating n-grams up to 5-grams, my system assesses and selects corrections that align naturally within the sentence's broader context, significantly enhancing correction accuracy for contextually misplaced words.\n\n## Dynamic Data Utilization\n\n#### Key Improvement\nDynamic generation and use of 3 and 4-gram data from 5-grams.\n\n#### Justification\nNorvig's approach relies on a fixed dataset for correction suggestions, focusing on individual word probabilities and edit distances. My method dynamically slices 5-grams to generate 3 and 4-grams, providing a more comprehensive understanding of language patterns without needing separately maintained datasets. This improves performance and scalability.\n\n## Enhanced Decision Making\n\n#### Key Improvement\nPrioritization of corrections based on context length and frequency.\n\n#### Justification\nMy solution evaluates candidate words based on their context length and occurrence, ensuring that chosen corrections are not only probable but also contextually coherent. This addresses the issue of correct words feeling \"out of place,\" a limitation in Norvig's simpler probability-based selection.\n\n## Flexible Correction Approach\n\n#### Key Improvement\nCorrection suggestions for known words based on edit distance and context.\n\n#### Justification\nNorvig's corrector accepts a known word as is, without considering potential contextual misfits. MY approach generates one-edit distance candidates even for known words, introducing flexibility and enabling the system to suggest alternatives that might better fit the context.","metadata":{"id":"6Xb_twOmVsC6"}},{"cell_type":"markdown","source":"# Evaluate on a test set","metadata":{"id":"46rk65S4GRSe"}},{"cell_type":"code","source":"from tqdm import tqdm\n\ndef correct_text(corrector, text):\n    \"\"\"Corrects a sentence text using the specified corrector.\"\"\"\n    return corrector.correct_sentence(text)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluation on highly auhmented test set","metadata":{}},{"cell_type":"code","source":"import pandas as pd \n\ntest_df = pd.read_csv('test.csv')\ntest_df = test_df.sample(500)\ntest_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:18:04.242881Z","iopub.execute_input":"2024-03-22T10:18:04.243264Z","iopub.status.idle":"2024-03-22T10:18:04.455914Z","shell.execute_reply.started":"2024-03-22T10:18:04.243237Z","shell.execute_reply":"2024-03-22T10:18:04.454977Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"                                                     text  \\\n3760              vic armed offenders squad above the law   \n34462   gold coast police rescues a man and woman off ...   \n100482  high end rental market slumps as mining boom t...   \n40346   penfold wants new visa category to attract mig...   \n90677                     six legged lamb born in belgium   \n\n                                           augmented_text  \n3760              vic arN$d offenders s!uaw abpvs the law  \n34462   gold coast ploiec rsceues a man and woman off ...  \n100482  hUisgh end rental market sOluEmps as mining bS...  \n40346   epfnold wants new vais category to attract mgi...  \n90677                     six legged mlab born in belgium  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>augmented_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3760</th>\n      <td>vic armed offenders squad above the law</td>\n      <td>vic arN$d offenders s!uaw abpvs the law</td>\n    </tr>\n    <tr>\n      <th>34462</th>\n      <td>gold coast police rescues a man and woman off ...</td>\n      <td>gold coast ploiec rsceues a man and woman off ...</td>\n    </tr>\n    <tr>\n      <th>100482</th>\n      <td>high end rental market slumps as mining boom t...</td>\n      <td>hUisgh end rental market sOluEmps as mining bS...</td>\n    </tr>\n    <tr>\n      <th>40346</th>\n      <td>penfold wants new visa category to attract mig...</td>\n      <td>epfnold wants new vais category to attract mgi...</td>\n    </tr>\n    <tr>\n      <th>90677</th>\n      <td>six legged lamb born in belgium</td>\n      <td>six legged mlab born in belgium</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"def word_by_word_accuracy(corrector, test_df):\n    \"\"\"Evaluate the spell corrector on a word-by-word basis.\"\"\"\n    total_words = 0\n    correct_matches = 0\n    \n    for _, row in tqdm(test_df.iterrows(), position=0, leave=False):\n        original_text = row['text'].split()\n        augmented_text = row['augmented_text']\n        \n        corrected_text = correct_text(corrector, augmented_text).split()\n        \n        for original_word, corrected_word in zip(original_text, corrected_text):\n            total_words += 1\n            if original_word == corrected_word:\n                correct_matches += 1\n\n    accuracy = correct_matches / total_words if total_words > 0 else 0\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:30:20.679748Z","iopub.execute_input":"2024-03-22T10:30:20.680126Z","iopub.status.idle":"2024-03-22T10:30:20.687206Z","shell.execute_reply.started":"2024-03-22T10:30:20.680096Z","shell.execute_reply":"2024-03-22T10:30:20.686082Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"accuracy_norvig_corrector = word_by_word_accuracy(spell_corrector, test_df)\naccuracy_context_corrector = word_by_word_accuracy(word_refiner, test_df)\n\nprint(f\"Context Sensetive Corrector Accuracy: {accuracy_context_corrector:.2f}\")\nprint(f\"Norvig's Corrector Accuracy: {accuracy_norvig_corrector:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:30:21.899260Z","iopub.execute_input":"2024-03-22T10:30:21.900120Z","iopub.status.idle":"2024-03-22T10:36:47.159254Z","shell.execute_reply.started":"2024-03-22T10:30:21.900089Z","shell.execute_reply":"2024-03-22T10:36:47.158349Z"},"trusted":true},"execution_count":67,"outputs":[{"name":"stderr","text":"                         ","output_type":"stream"},{"name":"stdout","text":"Context Sensetive Corrector Accuracy: 0.28\nNorvig's Corrector Accuracy: 0.20\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}]},{"cell_type":"markdown","source":"## Evaluation on auhmented test set with a little noise ","metadata":{}},{"cell_type":"code","source":"test_df2 = pd.read_csv(\"chatgpt_prompts.csv\")\ntest_df2 = test_df2.prompt\ntest_df2.head()","metadata":{"execution":{"iopub.status.busy":"2024-03-22T09:16:00.988528Z","iopub.execute_input":"2024-03-22T09:16:00.988932Z","iopub.status.idle":"2024-03-22T09:16:01.002717Z","shell.execute_reply.started":"2024-03-22T09:16:00.988902Z","shell.execute_reply":"2024-03-22T09:16:01.001895Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"0    I want you to act as my personal chef. I will ...\n1    I want you to act like a php interpreter. I wi...\n2    I want you act as a proofreader. I will provid...\n3    [Caveat Emptor: After issuing this prompt you ...\n4    I want you to act as a salesperson. Try to mar...\nName: prompt, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"import random\n\ndef introduce_typos(text, noise_level=0.1):\n    typo_text = []\n    for word in text.split():\n        if random.random() < noise_level:\n            # Introduce a simple typo: Swap two adjacent characters\n            if len(word) > 1:\n                pos = random.randint(0, len(word) - 2)\n                word = word[:pos] + word[pos+1] + word[pos] + word[pos+2:]\n        typo_text.append(word)\n    return ' '.join(typo_text)\n\ndef evaluate_accuracy(corrector, original_texts):\n    correct_count = 0\n    total_words = 0\n    for original_text in tqdm(original_texts, desc=f\"Evaluating\"):\n        typo_text = introduce_typos(original_text, noise_level=0.2) \n        corrected_text = correct_text(corrector, typo_text)\n        original_words = original_text.split()\n        corrected_words = corrected_text.split()\n        \n        total_words += len(original_words)\n        correct_count += sum(1 for orig, corr in zip(original_words, corrected_words) if orig == corr)\n    \n    accuracy = correct_count / total_words if total_words > 0 else 0\n    return accuracy","metadata":{"execution":{"iopub.status.busy":"2024-03-22T09:46:32.810107Z","iopub.execute_input":"2024-03-22T09:46:32.810713Z","iopub.status.idle":"2024-03-22T09:46:32.819349Z","shell.execute_reply.started":"2024-03-22T09:46:32.810682Z","shell.execute_reply":"2024-03-22T09:46:32.818390Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"accuracy_context_corrector = evaluate_accuracy(word_refiner, test_df2)\naccuracy_norvig_corrector = evaluate_accuracy(spell_corrector, test_df2)","metadata":{"execution":{"iopub.status.busy":"2024-03-22T09:46:37.130930Z","iopub.execute_input":"2024-03-22T09:46:37.131287Z","iopub.status.idle":"2024-03-22T10:10:31.314841Z","shell.execute_reply.started":"2024-03-22T09:46:37.131259Z","shell.execute_reply":"2024-03-22T10:10:31.313975Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [00:02<00:00, 53.88it/s]\nEvaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 153/153 [23:51<00:00,  9.36s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"print(f\"Context Sensetive Corrector Accuracy: {accuracy_context_corrector:.2f}\")\nprint(f\"Norvig's Corrector Accuracy: {accuracy_norvig_corrector:.2f}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-22T10:10:31.316948Z","iopub.execute_input":"2024-03-22T10:10:31.317596Z","iopub.status.idle":"2024-03-22T10:10:31.322590Z","shell.execute_reply.started":"2024-03-22T10:10:31.317560Z","shell.execute_reply":"2024-03-22T10:10:31.321641Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"Context Sensetive Corrector Accuracy: 0.51\nNorvig's Corrector Accuracy: 0.11\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here are the results of the evaluation:\n\n#### Evaluation on highly augmented test set:\n- Context Sensitive Corrector Accuracy: 28%\n- Norvig's Corrector Accuracy: 20%\n\n#### Evaluation on augmented test set with a little noise:\n- Context Sensitive Corrector Accuracy: 51%\n- Norvig's Corrector Accuracy: 11%\n\nFrom the results, it's clear that the Context Sensitive Corrector performs better than Norvig's Corrector in both test sets. The overall accuracy is higher for Context Sensitive Corrector, especially on the test set with a little noise. The Context Sensitive Corrector performs better than Norvig's Corrector for several reasons:\n\n1. **Contextual Awareness**: The Context Sensitive Corrector takes into account the context of the surrounding words in a sentence. Instead of suggesting corrections based solely on their similarity to the incorrect word, the corrector considers how well the suggestions fit within the sentence's broader context.\n\n2. **Dynamic Data Utilization**: The Context Sensitive Corrector dynamically generates 3 and 4-grams from a 5-gram dataset.\n\n3. **Enhanced Decision Making**: The Context Sensitive Corrector evaluates potential corrections based on their context length and frequency, ensuring that suggested corrections are not only probable but also contextually coherent. \n\n4. **Flexible Correction Approach**: The Context Sensitive Corrector offers correction suggestions for both misspelled and known words based on edit distance and context. Instead of accepting a known word as correct without considering the context, the Context Sensitive Corrector system takes both factors into account. \n\n","metadata":{}}]}